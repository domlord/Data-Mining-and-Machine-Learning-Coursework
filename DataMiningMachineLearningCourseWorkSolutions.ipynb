{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa42a1b-7834-453b-b54e-b244b6bd0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Please write the optimal hyperparameter values you obtain in the global variable 'optimal_hyperparm' below. This\n",
    "# variable should contain the values when I look at your submission. I should not have to run your code to populate this\n",
    "# variable.\n",
    "optimal_hyperparam = {}\n",
    "\n",
    "class COC131:\n",
    " def q1(self, filename=None):\n",
    "    \"\"\"\n",
    "    This function should be used to load the data. To speed-up processing in later steps, lower resolution of the\n",
    "    image to 32*32. The folder names in the root directory of the dataset are the class names. After loading the\n",
    "    dataset, you should save it into an instance variable self.x (for samples) and self.y (for labels). Both self.x\n",
    "    and self.y should be numpy arrays of dtype float.\n",
    "\n",
    "    :param filename: this is the name of an actual random image in the dataset. You don't need this to load the\n",
    "    dataset. This is used for testing the implementation.\n",
    "    :return res1: a one-dimensional numpy array containing the flattened low-resolution image in file 'filename'.\n",
    "    Flatten the image in the row major order. The dtype for the array should be float.\n",
    "    :return res2: a string containing the class name for the image in file 'filename'. This string should be the same as\n",
    "    one of the folder names in the originally shared dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get all subfolders within the dataset directory\n",
    "    subfolders = [entry.path for entry in os.scandir(\"../dataset\") if entry.is_dir()]\n",
    "\n",
    "    # Creates a list of all paths for the images stored within the ../dataset directory\n",
    "    image_paths = [os.path.join(subfolder, file)\n",
    "                 for subfolder in subfolders\n",
    "                 for file in os.listdir(subfolder) if file.endswith(('.jpg'))] # If the file is an image\n",
    "\n",
    "    # Processes each image within the dataset directory and stores it into the images array\n",
    "    self.x = np.array([np.array(Image.open(image_path).resize((32, 32)), dtype=float).flatten() for image_path in image_paths])\n",
    "\n",
    "    # Gets the name of the folder that the image is stored in (the classification of the image)\n",
    "    self.y = np.array([os.path.basename(os.path.dirname(image_path)) for image_path in image_paths])\n",
    "\n",
    "    # If a filename is provided, find and return it\n",
    "    if filename:\n",
    "        matching_image_path = next((image_path for image_path in image_paths if filename in image_path), None)\n",
    "        \n",
    "        if matching_image_path:\n",
    "            res1 = np.array(Image.open(matching_image_path).resize((32, 32)), dtype=float).flatten()\n",
    "            res2 = os.path.basename(os.path.dirname(matching_image_path))  # Extract class name\n",
    "            return res1, res2\n",
    "\n",
    "    return self.x, self.y\n",
    "\n",
    "\n",
    "\n",
    "    def q2(self, inp):\n",
    "        \"\"\"\n",
    "        This function should compute the standardized data from a given 'inp' data. The function should work for a\n",
    "        dataset with any number of features.\n",
    "\n",
    "        :param inp: an array from which the standardized data is to be computed.\n",
    "        :return res2: a numpy array containing the standardized data with standard deviation of 2.5. The array should\n",
    "        have the same dimensions as the original data\n",
    "        :return res1: sklearn object used for standardization.\n",
    "        \"\"\"\n",
    "\n",
    "        standard_scaler = StandardScaler()\n",
    "        standardised_data = standardScaler.fit_transform(inp)\n",
    "\n",
    "        res1 = standard_scaler\n",
    "        res2 = standardised_data * 2.5\n",
    "\n",
    "        return res2, res1\n",
    "\n",
    "    def q3(self, test_size=None, pre_split_data=None, hyperparam=None):\n",
    "        \"\"\"\n",
    "        This function should build a MLP Classifier using the dataset loaded in function 'q1' and evaluate model\n",
    "        performance. You can assume that the function 'q1' has been called prior to calling this function. This function\n",
    "        should support hyperparameter optimizations.\n",
    "\n",
    "        :param test_size: the proportion of the dataset that should be reserved for testing. This should be a fraction\n",
    "        between 0 and 1.\n",
    "        :param pre_split_data: Can be used to provide data already split into training and testing.\n",
    "        :param hyperparam: hyperparameter values to be tested during hyperparameter optimization.\n",
    "        :return: The function should return 1 model object and 3 numpy arrays which contain the loss, training accuracy\n",
    "        and testing accuracy after each training iteration for the best model you found.\n",
    "        \"\"\"\n",
    "\n",
    "        # normalize data\n",
    "\n",
    "        res1 = object()\n",
    "        res2 = np.zeros(1)\n",
    "        res3 = np.zeros(1)\n",
    "        res4 = np.zeros(1)\n",
    "\n",
    "        return res1, res2, res3, res4\n",
    "\n",
    "    def q4(self):\n",
    "        \"\"\"\n",
    "        This function should study the impact of alpha on the performance and parameters of the model. For each value of\n",
    "        alpha in the list below, train a separate MLPClassifier from scratch. Other hyperparameters for the model can\n",
    "        be set to the best values you found in 'q3'. You can assume that the function 'q1' has been called\n",
    "        prior to calling this function.\n",
    "\n",
    "        :return: res should be the data you visualized.\n",
    "        \"\"\"\n",
    "\n",
    "        alpha_values = [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 50, 100]\n",
    "\n",
    "        res = np.zeros(1)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def q5(self):\n",
    "        \"\"\"\n",
    "        This function should perform hypothesis testing to study the impact of using CV with and without Stratification\n",
    "        on the performance of MLPClassifier. Set other model hyperparameters to the best values obtained in the previous\n",
    "        questions. Use 5-fold cross validation for this question. You can assume that the function 'q1' has been called\n",
    "        prior to calling this function.\n",
    "\n",
    "        :return: The function should return 4 items - the final testing accuracy for both methods of CV, p-value of the\n",
    "        test and a string representing the result of hypothesis testing. The string can have only two possible values -\n",
    "        'Splitting method impacted performance' and 'Splitting method had no effect'.\n",
    "        \"\"\"\n",
    "\n",
    "        res1 = 0\n",
    "        res2 = 0\n",
    "        res3 = 0\n",
    "        res4 = ''\n",
    "\n",
    "        return res1, res2, res3, res4\n",
    "\n",
    "    def q6(self):\n",
    "        \"\"\"\n",
    "        This function should perform unsupervised learning using LocallyLinearEmbedding in Sklearn. You can assume that\n",
    "        the function 'q1' has been called prior to calling this function.\n",
    "\n",
    "        :return: The function should return the data you visualize.\n",
    "        \"\"\"\n",
    "\n",
    "        res = np.zeros(1)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5fc807a-43ce-45a4-8eaf-81f17dd4614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest\n",
      "[[148. 121. 120. ... 106.  94. 104.]\n",
      " [ 56.  98.  83. ... 173. 127. 112.]\n",
      " [ 69.  93.  96. ... 204. 160. 150.]\n",
      " ...\n",
      " [ 32.  55.  80. ...  32.  53.  81.]\n",
      " [ 54.  71.  80. ...  53.  73.  82.]\n",
      " [102.  99. 108. ...  96.  96. 106.]]\n",
      "['AnnualCrop' 'AnnualCrop' 'AnnualCrop' ... 'SeaLake' 'SeaLake' 'SeaLake']\n"
     ]
    }
   ],
   "source": [
    "dataset = COC131()\n",
    "image_array, class_label = dataset.q1(\"Forest_1.jpg\")  # Change to an actual filename in your dataset\n",
    "print(class_label)\n",
    "images, labels = dataset.q1()\n",
    "print(images)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bcda5e-a7a9-4456-bbb7-b01ae13a3139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322c14b-0cdc-403e-b678-b36f14efe9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
