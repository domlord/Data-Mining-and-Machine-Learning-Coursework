{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffa42a1b-7834-453b-b54e-b244b6bd0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Please write the optimal hyperparameter values you obtain in the global variable 'optimal_hyperparm' below. This\n",
    "# variable should contain the values when I look at your submission. I should not have to run your code to populate this\n",
    "# variable.\n",
    "optimal_hyperparam = {}\n",
    "\n",
    "class COC131:\n",
    "    def q1(self, filename=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function should be used to load the data. To speed-up processing in later steps, lower resolution of the\n",
    "        image to 32*32. The folder names in the root directory of the dataset are the class names. After loading the\n",
    "        dataset, you should save it into an instance variable self.x (for samples) and self.y (for labels). Both self.x\n",
    "        and self.y should be numpy arrays of dtype float.\n",
    "    \n",
    "        :param filename: this is the name of an actual random image in the dataset. You don't need this to load the\n",
    "        dataset. This is used for testing the implementation.\n",
    "        :return res1: a one-dimensional numpy array containing the flattened low-resolution image in file 'filename'.\n",
    "        Flatten the image in the row major order. The dtype for the array should be float.\n",
    "        :return res2: a string containing the class name for the image in file 'filename'. This string should be the same as\n",
    "        one of the folder names in the originally shared dataset.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Get all subfolders within the dataset directory\n",
    "        subfolders = [entry.path for entry in os.scandir(\"../dataset\") if entry.is_dir()]\n",
    "    \n",
    "        # Creates a list of all paths for the images stored within the ../dataset directory\n",
    "        image_paths = [os.path.join(subfolder, file)\n",
    "                     for subfolder in subfolders\n",
    "                     for file in os.listdir(subfolder) if file.endswith(('.jpg'))] # If the file is an image\n",
    "    \n",
    "        # Processes each image within the dataset directory and stores it into the images array\n",
    "        self.x = np.array([np.array(Image.open(image_path).resize((32, 32)), dtype=float).flatten() for image_path in image_paths])\n",
    "    \n",
    "        # Gets the name of the folder that the image is stored in (the classification of the image)\n",
    "        self.y = np.array([os.path.basename(os.path.dirname(image_path)) for image_path in image_paths])\n",
    "    \n",
    "        # If a filename is provided, find and return it\n",
    "        if filename:\n",
    "            matching_image_path = next((image_path for image_path in image_paths if filename in image_path), None)\n",
    "            \n",
    "            if matching_image_path:\n",
    "                res1 = np.array(Image.open(matching_image_path).resize((32, 32)), dtype=float).flatten()\n",
    "                res2 = os.path.basename(os.path.dirname(matching_image_path))  # Extract class name\n",
    "                return res1, res2\n",
    "                \n",
    "        return self.x, self.y\n",
    "    \n",
    "    def q2(self, inp):\n",
    "        \"\"\"\n",
    "        This function should compute the standardized data from a given 'inp' data. The function should work for a\n",
    "        dataset with any number of features.\n",
    "\n",
    "        :param inp: an array from which the standardized data is to be computed.\n",
    "        :return res2: a numpy array containing the standardized data with standard deviation of 2.5. The array should\n",
    "        have the same dimensions as the original data\n",
    "        :return res1: sklearn object used for standardization.\n",
    "        \"\"\"\n",
    "\n",
    "        standard_scaler = StandardScaler() # Creates a sklearn object used for standardisation\n",
    "        standardised_data = standardScaler.fit_transform(inp) # Standardises the data (setting the standard deviation to 1)\n",
    "\n",
    "        res1 = standard_scaler\n",
    "        res2 = standardised_data * 2.5 # Standardises the data to have a standard deviation of 2.5\n",
    "\n",
    "        return res2, res1\n",
    "    \n",
    "    \n",
    "    def visualise_standardisation(original_data, standardised_data):\n",
    "         \n",
    "         \"\"\"\n",
    "         Visualises original and standardised data side by side using boxplots.\n",
    "         :param original_data: numpy array of original dataset\n",
    "         :param standardised_data: numpy array of standardised dataset\n",
    "         \"\"\"\n",
    "         fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "         axs[0].boxplot(original_data, vert=True)\n",
    "         axs[0].set_title(\"Original Data Distribution\")\n",
    "         axs[0].set_xlabel(\"Features\")\n",
    "         axs[0].set_ylabel(\"Value\")\n",
    "    \n",
    "         axs[1].boxplot(standardized_data, vert=True)\n",
    "         axs[1].set_title(\"Standardised Data Distribution (std = 2.5)\")\n",
    "         axs[1].set_xlabel(\"Features\")\n",
    "         axs[1].set_ylabel(\"Value\")\n",
    "         \n",
    "    \n",
    "         plt.tight_layout()\n",
    "         plt.show()\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "     \n",
    "    def visualise_hyperparam_results(param_labels, cv_scores):\n",
    "        \"\"\"\n",
    "        Generates a horizontal bar chart to visualise cross-validation accuracy \n",
    "        for each hyperparameter combination.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(param_labels, cv_scores, color='skyblue')\n",
    "        plt.xlabel(\"Mean Cross-Validation Accuracy\")\n",
    "        plt.title(\"Hyperparameter Optimization Results\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def q3(self, test_size=None, pre_split_data=None, hyperparam=None):\n",
    "        \"\"\"\n",
    "        Trains an MLPClassifier on a dataset, optionally performs hyperparameter optimization,\n",
    "        and visualizes the results.\n",
    "        \"\"\"\n",
    "        if self.scaler is None:\n",
    "            self.x, self.scaler = self.q2(self.x)\n",
    "    \n",
    "        if test_size is None:\n",
    "            test_size = 0.3\n",
    "    \n",
    "        if pre_split_data:\n",
    "            x_train, x_test, y_train, y_test = pre_split_data\n",
    "        else:\n",
    "            x_train, x_test, y_train, y_test = train_test_split(self.x, self.y, test_size=test_size, random_state=1)\n",
    "    \n",
    "        if hyperparam:\n",
    "            model = MLPClassifier(**hyperparam, random_state=1)\n",
    "            model.fit(x_train, y_train)\n",
    "            self.best_hyperparams = hyperparam\n",
    "        else:\n",
    "            param_grid = {\n",
    "                \"hidden_layer_sizes\": [(50,), (100,), (100, 50), (200,)],\n",
    "                \"learning_rate\": [\"constant\", \"adaptive\"],\n",
    "                \"solver\": [\"adam\", \"sgd\"]\n",
    "            }\n",
    "    \n",
    "            grid_search = GridSearchCV(\n",
    "                MLPClassifier(random_state=1),\n",
    "                param_grid,\n",
    "                cv=3\n",
    "            )\n",
    "            grid_search.fit(x_train, y_train)\n",
    "    \n",
    "            # Visualization step\n",
    "            param_labels = [str(p) for p in grid_search.cv_results_[\"params\"]]\n",
    "            cv_scores = grid_search.cv_results_[\"mean_test_score\"]\n",
    "            visualize_hyperparam_results(param_labels, cv_scores)\n",
    "    \n",
    "            model = grid_search.best_estimator_\n",
    "            self.best_hyperparams = grid_search.best_params_\n",
    "    \n",
    "        loss_curve = np.array(model.loss_curve_) if hasattr(model, \"loss_curve_\") else np.array([])\n",
    "        train_accuracy = np.array([accuracy_score(y_train, model.predict(x_train))])\n",
    "        test_accuracy = np.array([accuracy_score(y_test, model.predict(x_test))])\n",
    "    \n",
    "        return model, loss_curve, train_accuracy, test_accuracy\n",
    "    \n",
    "    def q4(self):\n",
    "        \"\"\"\n",
    "        This function should study the impact of alpha on the performance and parameters of the model. For each value of\n",
    "        alpha in the list below, train a separate MLPClassifier from scratch. Other hyperparameters for the model can\n",
    "        be set to the best values you found in 'q3'. You can assume that the function 'q1' has been called\n",
    "        prior to calling this function.\n",
    "\n",
    "        :return: res should be the data you visualized.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Train/test split (same as q3)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(self.x, self.y, test_size=0.3, random_state=1)\n",
    "    \n",
    "        # Alpha values to test\n",
    "        alpha_values = [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 50, 100]\n",
    "    \n",
    "        train_accuracies = []\n",
    "        test_accuracies = []\n",
    "    \n",
    "        # Train a model for each alpha value\n",
    "        for alpha in alpha_values:\n",
    "            # Use best hyperparameters from q3(), but override alpha\n",
    "            hyperparams = self.best_hyperparams.copy()\n",
    "            hyperparams[\"alpha\"] = alpha\n",
    "    \n",
    "            model = MLPClassifier(**hyperparams, random_state=1)\n",
    "            model.fit(X_train, y_train)\n",
    "    \n",
    "            # Evaluate model\n",
    "            train_acc = accuracy_score(y_train, model.predict(x_train))\n",
    "            test_acc = accuracy_score(y_test, model.predict(x_test))\n",
    "    \n",
    "            train_accuracies.append(train_acc)\n",
    "            test_accuracies.append(test_acc)\n",
    "    \n",
    "            # Model parameters\n",
    "            weights = model.coefs_\n",
    "            biases = model.intercepts_\n",
    "\n",
    "            total_w = sum(w.size for w in weights)\n",
    "            total_b = sum(b.size for b in biases)\n",
    "    \n",
    "            total_weights.append(total_w)\n",
    "            total_biases.append(total_b)\n",
    "\n",
    "        # Store results in a dictionary\n",
    "        res = {\n",
    "        \"alpha_values\": alpha_values,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"test_accuracies\": test_accuracies,\n",
    "        \"total_weights\": total_weights,\n",
    "        \"total_biases\": total_biases,\n",
    "        }\n",
    "        return res\n",
    "    \n",
    "\n",
    "    \n",
    "    def visualize_alpha_impact(res):\n",
    "         \n",
    "        \"\"\"\n",
    "        Visualizes how alpha (regularization strength) affects model performance and parameters.\n",
    "        \n",
    "        : param res: Dictionary output from q4() containing accuracy and parameter data.\n",
    "        \"\"\"\n",
    "        alphas = res[\"alpha_values\"]\n",
    "        train_acc = res[\"train_accuracies\"]\n",
    "        test_acc = res[\"test_accuracies\"]\n",
    "        weights = res[\"total_weights\"]\n",
    "        biases = res[\"total_biases\"]\n",
    "    \n",
    "        fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "        # Plot Accuracy vs Alpha\n",
    "        axs[0].plot(alphas, train_acc, marker='o', label='Train Accuracy')\n",
    "        axs[0].plot(alphas, test_acc, marker='s', label='Test Accuracy')\n",
    "        axs[0].set_xscale('log')\n",
    "        axs[0].set_xlabel(\"Alpha (L2 Regularization Strength)\")\n",
    "        axs[0].set_ylabel(\"Accuracy\")\n",
    "        axs[0].set_title(\"Effect of Alpha on Model Accuracy\")\n",
    "        axs[0].legend()\n",
    "        axs[0].grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "        # Plot Parameters vs Alpha\n",
    "        axs[1].plot(alphas, weights, marker='o', label='Total Weights')\n",
    "        axs[1].plot(alphas, biases, marker='s', label='Total Biases')\n",
    "        axs[1].set_xscale('log')\n",
    "        axs[1].set_xlabel(\"Alpha (L2 Regularization Strength)\")\n",
    "        axs[1].set_ylabel(\"Parameter Count\")\n",
    "        axs[1].set_title(\"Effect of Alpha on Model Parameters\")\n",
    "        axs[1].legend()\n",
    "        axs[1].grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    def q5(self):\n",
    "        \"\"\"\n",
    "        This function performs hypothesis testing to study the impact of using CV with and without Stratification\n",
    "        on the performance of MLPClassifier. Set other model hyperparameters to the best values obtained in the previous\n",
    "        questions. Use 5-fold cross validation for this question. You can assume that the function 'q1' has been called\n",
    "        prior to calling this function.\n",
    "    \n",
    "        :return: The function returns 4 items - the final testing accuracy for both methods of CV, p-value of the\n",
    "                 test and a string representing the result of hypothesis testing. The string can have only two possible values:\n",
    "                 'Splitting method impacted performance' or 'Splitting method had no effect'.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Use best hyperparameters from q3()\n",
    "        hyperparams = self.best_hyperparams.copy()\n",
    "    \n",
    "        # Define the model\n",
    "        model = MLPClassifier(**hyperparams, random_state=1)\n",
    "    \n",
    "        # Define 5-Fold CV (Without Stratification)\n",
    "        kf = KFold(shuffle=True, random_state=1)\n",
    "        cross_value_scores_kf = cross_val_score(model, self.x, self.y, cv=kf, n_jobs=-1)\n",
    "    \n",
    "        # Define 5-Fold Stratified CV\n",
    "        skf = StratifiedKFold(shuffle=True, random_state=1)\n",
    "        cross_value_scores_skf = cross_val_score(model, self.x, self.y, cv=skf, n_jobs=-1)\n",
    "    \n",
    "        # Compute mean testing accuracy\n",
    "        mean_accuracy_kf = np.mean(cv_scores_kf)\n",
    "        mean_accuracy_skf = np.mean(cv_scores_skf)\n",
    "    \n",
    "        # Perform paired t-test to compare distributions\n",
    "        t_stat, p_value = ttest_rel(cv_scores_kf, cv_scores_skf)\n",
    "    \n",
    "        # Determine hypothesis test result\n",
    "        alpha = 0.05  # Significance level\n",
    "        if p_value < alpha:\n",
    "            hypothesis_result = \"Splitting method impacted performance\"\n",
    "        else:\n",
    "            hypothesis_result = \"Splitting method had no effect\"\n",
    "    \n",
    "        return mean_acc_kf, mean_acc_skf, p_value, hypothesis_result\n",
    "\n",
    "    def visualise_cv_comparison(scores_kf, scores_skf):\n",
    "        \"\"\"\n",
    "        Plots a boxplot comparing performance between K-Fold and Stratified K-Fold cross-validation.\n",
    "    \n",
    "        :param scores_kf: List or array of accuracy scores from K-Fold CV\n",
    "        :param scores_skf: List or array of accuracy scores from Stratified K-Fold CV\n",
    "        \"\"\"\n",
    "        data = [scores_kf, scores_skf]\n",
    "        labels = ['K-Fold CV', 'Stratified K-Fold CV']\n",
    "    \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.boxplot(data, labels=labels, patch_artist=True,\n",
    "                    boxprops=dict(facecolour='lightblue'),\n",
    "                    medianprops=dict(colour='black'))\n",
    "        plt.title('Cross-Validation Accuracy Comparison')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def q6(self):\n",
    "        \"\"\"\n",
    "        This function performs unsupervised learning using LocallyLinearEmbedding in sklearn to reduce the dataset\n",
    "        to 2 dimensions. It returns the embedded data and corresponding labels to enable visualization of class separability.\n",
    "    \n",
    "        :return: Dictionary containing 2D embedded data and class labels.\n",
    "        \"\"\"\n",
    "    \n",
    "        if self.x is None or self.y is None:\n",
    "            raise ValueError(\"Dataset not loaded. Please call q1() first.\")\n",
    "    \n",
    "        # Apply LLE with custom n_neighbors, but default 2D output\n",
    "        lle = LocallyLinearEmbedding(n_neighbors=10, random_state=1)\n",
    "        embedded_data = lle.fit_transform(self.x)\n",
    "    \n",
    "        res = {\n",
    "            \"embedded_data\": embedded_data,\n",
    "            \"labels\": self.y\n",
    "        }\n",
    "    \n",
    "        return res\n",
    "\n",
    "    def visualise_lle_separability(embedded_data, labels):\n",
    "        \"\"\"\n",
    "        Plots a 2D scatterplot of the LLE embedded data, coloured by class.\n",
    "    \n",
    "        :param embedded_data: 2D numpy array from LLE\n",
    "        :param labels: Class labels corresponding to each point\n",
    "        \"\"\"\n",
    "        labels = np.array(labels)\n",
    "        unique_classes = np.unique(labels)\n",
    "        colours = plt.cm.get_cmap('tab10', len(unique_classes))\n",
    "    \n",
    "        plt.figure(figsize=(10, 7))\n",
    "        for i, cls in enumerate(unique_classes):\n",
    "            idx = labels == cls\n",
    "            plt.scatter(embedded_data[idx, 0], embedded_data[idx, 1],\n",
    "                        label=cls, alpha=0.7, s=30, c=[colours(i)])\n",
    "    \n",
    "        plt.title(\"2D Class Visualisation Using Locally Linear Embedding\")\n",
    "        plt.xlabel(\"LLE Dimension 1\")\n",
    "        plt.ylabel(\"LLE Dimension 2\")\n",
    "        plt.legend(title=\"Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5322c14b-0cdc-403e-b678-b36f14efe9b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m      2\u001b[39m model = COC131()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mq1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[32m      5\u001b[39m standardised_x, scaler = model.q2(model.x)\n\u001b[32m      6\u001b[39m model.visualise_standardisation(model.x, standardised_x)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mCOC131.q1\u001b[39m\u001b[34m(self, filename)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03mThis function should be used to load the data. To speed-up processing in later steps, lower resolution of the\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03mimage to 32*32. The folder names in the root directory of the dataset are the class names. After loading the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m \u001b[33;03mone of the folder names in the originally shared dataset.\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Get all subfolders within the dataset directory\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m subfolders = [entry.path \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../dataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry.is_dir()]\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Creates a list of all paths for the images stored within the ../dataset directory\u001b[39;00m\n\u001b[32m     36\u001b[39m image_paths = [os.path.join(subfolder, file)\n\u001b[32m     37\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m subfolder \u001b[38;5;129;01min\u001b[39;00m subfolders\n\u001b[32m     38\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os.listdir(subfolder) \u001b[38;5;28;01mif\u001b[39;00m file.endswith((\u001b[33m'\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m'\u001b[39m))] \u001b[38;5;66;03m# If the file is an image\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: '../dataset'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "model = COC131()\n",
    "model.q1()  # Load the data\n",
    "\n",
    "standardised_x, scaler = model.q2(model.x)\n",
    "model.visualise_standardisation(model.x, standardised_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbad92a-2a2f-47ae-b854-2f42d0768f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
