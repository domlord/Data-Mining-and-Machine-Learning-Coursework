{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffa42a1b-7834-453b-b54e-b244b6bd0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Please write the optimal hyperparameter values you obtain in the global variable 'optimal_hyperparm' below. This\n",
    "# variable should contain the values when I look at your submission. I should not have to run your code to populate this\n",
    "# variable.\n",
    "optimal_hyperparam = {}\n",
    "\n",
    "class COC131:\n",
    " def q1(self, filename=None):\n",
    "    \"\"\"\n",
    "    This function should be used to load the data. To speed-up processing in later steps, lower resolution of the\n",
    "    image to 32*32. The folder names in the root directory of the dataset are the class names. After loading the\n",
    "    dataset, you should save it into an instance variable self.x (for samples) and self.y (for labels). Both self.x\n",
    "    and self.y should be numpy arrays of dtype float.\n",
    "\n",
    "    :param filename: this is the name of an actual random image in the dataset. You don't need this to load the\n",
    "    dataset. This is used for testing the implementation.\n",
    "    :return res1: a one-dimensional numpy array containing the flattened low-resolution image in file 'filename'.\n",
    "    Flatten the image in the row major order. The dtype for the array should be float.\n",
    "    :return res2: a string containing the class name for the image in file 'filename'. This string should be the same as\n",
    "    one of the folder names in the originally shared dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get all subfolders within the dataset directory\n",
    "    subfolders = [entry.path for entry in os.scandir(\"../dataset\") if entry.is_dir()]\n",
    "\n",
    "    # Creates a list of all paths for the images stored within the ../dataset directory\n",
    "    image_paths = [os.path.join(subfolder, file)\n",
    "                 for subfolder in subfolders\n",
    "                 for file in os.listdir(subfolder) if file.endswith(('.jpg'))] # If the file is an image\n",
    "\n",
    "    # Processes each image within the dataset directory and stores it into the images array\n",
    "    self.x = np.array([np.array(Image.open(image_path).resize((32, 32)), dtype=float).flatten() for image_path in image_paths])\n",
    "\n",
    "    # Gets the name of the folder that the image is stored in (the classification of the image)\n",
    "    self.y = np.array([os.path.basename(os.path.dirname(image_path)) for image_path in image_paths])\n",
    "\n",
    "    # If a filename is provided, find and return it\n",
    "    if filename:\n",
    "        matching_image_path = next((image_path for image_path in image_paths if filename in image_path), None)\n",
    "        \n",
    "        if matching_image_path:\n",
    "            res1 = np.array(Image.open(matching_image_path).resize((32, 32)), dtype=float).flatten()\n",
    "            res2 = os.path.basename(os.path.dirname(matching_image_path))  # Extract class name\n",
    "            return res1, res2\n",
    "\n",
    "    return self.x, self.y\n",
    "\n",
    "\n",
    "\n",
    "    def q2(self, inp):\n",
    "        \"\"\"\n",
    "        This function should compute the standardized data from a given 'inp' data. The function should work for a\n",
    "        dataset with any number of features.\n",
    "\n",
    "        :param inp: an array from which the standardized data is to be computed.\n",
    "        :return res2: a numpy array containing the standardized data with standard deviation of 2.5. The array should\n",
    "        have the same dimensions as the original data\n",
    "        :return res1: sklearn object used for standardization.\n",
    "        \"\"\"\n",
    "\n",
    "        standard_scaler = StandardScaler() # Creates a sklearn object used for standardisation\n",
    "        standardised_data = standardScaler.fit_transform(inp) # Standardises the data (setting the standard deviation to 1)\n",
    "\n",
    "        res1 = standard_scaler\n",
    "        res2 = standardised_data * 2.5 # Standardises the data to have a standard deviation of 2.5\n",
    "\n",
    "        return res2, res1\n",
    "\n",
    "    def q3(self, test_size=None, pre_split_data=None, hyperparam=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function builds an MLP Classifier using the dataset loaded in function 'q1' and evaluates model\n",
    "        erformance. You can assume that the function 'q1' has been called prior to calling this function.\n",
    "        This function supports hyperparameter optimizations.\n",
    "        \n",
    "        :param test_size: The proportion of the dataset that should be reserved for testing. Should be a fraction between 0 and 1.\n",
    "        Default is 0.3 (30% for testing)\n",
    "        :param pre_split_data: Can be used to provide data already split into training and testing.\n",
    "        param hyperparam: Dictionary of hyperparameter values to be tested during optimization.\n",
    "        :return: The function returns 1 model object and 3 numpy arrays containing the loss, training accuracy,\n",
    "        and testing accuracy after each training iteration for the best model found.\n",
    "        \"\"\"\n",
    "        # Normalise the data using q2()\n",
    "        if self.scaler is None:\n",
    "            self.x, self.scaler = self.q2(self.x)\n",
    "\n",
    "        # Set default test size to 30% if none is provided\n",
    "        if test_size is None:\n",
    "            test_size = 0.3  # Default 70% training, 30% testing split\n",
    "    \n",
    "        # Train/test split if pre-split data is not provided\n",
    "        if pre_split_data:\n",
    "            x_train, x_test, y_train, y_test = pre_split_data\n",
    "        else:\n",
    "            x_train, x_test, y_train, y_test = train_test_split(self.x, self.y, test_size=test_size, random_state=1)\n",
    "    \n",
    "        # If hyperparameters are provided, use them directly\n",
    "        if hyperparam:\n",
    "            model = MLPClassifier(**hyperparam, random_state=1) # Set the random_state to 1 as a value to standardise the classifier generated\n",
    "            model.fit(x_train, y_train)\n",
    "            self.best_hyperparams = hyperparam  # Store the given hyperparameters\n",
    "        else:\n",
    "            # Define parameter grid for Grid Search (excluding alpha, which is tuned in q4)\n",
    "            param_grid = {\n",
    "                \"hidden_layer_sizes\": [(50,), (100,), (100, 50), (200,)],  # Different network sizes\n",
    "                \"learning_rate\": [\"constant\", \"adaptive\"],  # Learning rate type\n",
    "                \"solver\": [\"adam\", \"sgd\"]  # Optimisation solvers\n",
    "            }\n",
    "\n",
    "        # Initialize Grid Search with cross-validation\n",
    "        # This Grid Search creates a \n",
    "        grid_search = GridSearchCV(\n",
    "            MLPClassifier(random_state=1),  # Set the random_state to 1 as a value to standardise the classifier generated\n",
    "            param_grid)\n",
    "\n",
    "        # Perform Grid Search\n",
    "        grid_search.fit(x_train, y_train)\n",
    "\n",
    "        # Get the best model and hyperparameters\n",
    "        model = grid_search.best_estimator_\n",
    "        self.best_hyperparams = grid_search.best_params_\n",
    "    \n",
    "        # Extract performance metrics\n",
    "        loss_curve = np.array(model.loss_curve_)  # Loss during training\n",
    "        train_accuracy = np.array([accuracy_score(y_train, model.predict(x_train))])\n",
    "        test_accuracy = np.array([accuracy_score(y_test, model.predict(Xxtest))])\n",
    "    \n",
    "        return model, loss_curve, train_accuracy, test_accuracy\n",
    "\n",
    "    def q4(self):\n",
    "        \"\"\"\n",
    "        This function studies the impact of alpha (L2 regularization) on model performance.\n",
    "        It trains multiple MLP classifiers using different alpha values while keeping other \n",
    "        hyperparameters the same as the best found in q3().\n",
    "    \n",
    "        :return res: Dictionary containing accuracy scores for each alpha.\n",
    "        \"\"\"\n",
    "    \n",
    "        if self.x is None or self.y is None:\n",
    "            raise ValueError(\"Dataset not loaded. Please call q1() first.\")\n",
    "        \n",
    "        if not self.best_hyperparams:\n",
    "            raise ValueError(\"Best hyperparameters not found. Please run q3() first.\")\n",
    "    \n",
    "        # Train/test split (same as q3)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.x, self.y, test_size=0.3, random_state=1)\n",
    "    \n",
    "        # Alpha values to test\n",
    "        alpha_values = [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 50, 100]\n",
    "    \n",
    "        train_accuracies = []\n",
    "        test_accuracies = []\n",
    "    \n",
    "        # Train a model for each alpha value\n",
    "        for alpha in alpha_values:\n",
    "            # Use best hyperparameters from q3(), but override alpha\n",
    "            hyperparams = self.best_hyperparams.copy()\n",
    "            hyperparams[\"alpha\"] = alpha\n",
    "    \n",
    "            model = MLPClassifier(**hyperparams, random_state=1)\n",
    "            model.fit(X_train, y_train)\n",
    "    \n",
    "            # Evaluate model\n",
    "            train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "            test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    \n",
    "            train_accuracies.append(train_acc)\n",
    "            test_accuracies.append(test_acc)\n",
    "    \n",
    "        # Store results in a dictionary\n",
    "        res = {\n",
    "            \"alpha_values\": np.array(alpha_values),\n",
    "            \"train_accuracies\": np.array(train_accuracies),\n",
    "            \"test_accuracies\": np.array(test_accuracies)\n",
    "        }\n",
    "\n",
    "    return res\n",
    "\n",
    "    def q5(self):\n",
    "        \"\"\"\n",
    "        This function performs hypothesis testing to study the impact of using CV with and without Stratification\n",
    "        on the performance of MLPClassifier. Set other model hyperparameters to the best values obtained in the previous\n",
    "        questions. Use 5-fold cross validation for this question. You can assume that the function 'q1' has been called\n",
    "        prior to calling this function.\n",
    "    \n",
    "        :return: The function returns 4 items - the final testing accuracy for both methods of CV, p-value of the\n",
    "                 test and a string representing the result of hypothesis testing. The string can have only two possible values:\n",
    "                 'Splitting method impacted performance' or 'Splitting method had no effect'.\n",
    "        \"\"\"\n",
    "    \n",
    "        if self.x is None or self.y is None:\n",
    "            raise ValueError(\"Dataset not loaded. Please call q1() first.\")\n",
    "    \n",
    "        if not self.best_hyperparams:\n",
    "            raise ValueError(\"Best hyperparameters not found. Please run q3() first.\")\n",
    "    \n",
    "        # Use best hyperparameters from q3()\n",
    "        hyperparams = self.best_hyperparams.copy()\n",
    "    \n",
    "        # Define the model\n",
    "        model = MLPClassifier(**hyperparams, random_state=1)\n",
    "    \n",
    "        # Define 5-Fold CV (Without Stratification)\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "        cv_scores_kf = cross_val_score(model, self.x, self.y, cv=kf, scoring=\"accuracy\", n_jobs=-1)\n",
    "    \n",
    "        # Define 5-Fold Stratified CV\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "        cv_scores_skf = cross_val_score(model, self.x, self.y, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n",
    "    \n",
    "        # Compute mean testing accuracy\n",
    "        mean_acc_kf = np.mean(cv_scores_kf)\n",
    "        mean_acc_skf = np.mean(cv_scores_skf)\n",
    "    \n",
    "        # Perform paired t-test to compare distributions\n",
    "        t_stat, p_value = ttest_rel(cv_scores_kf, cv_scores_skf)\n",
    "    \n",
    "        # Determine hypothesis test result\n",
    "        alpha = 0.05  # Significance level\n",
    "        if p_value < alpha:\n",
    "            hypothesis_result = \"Splitting method impacted performance\"\n",
    "        else:\n",
    "            hypothesis_result = \"Splitting method had no effect\"\n",
    "    \n",
    "        return mean_acc_kf, mean_acc_skf, p_value, hypothesis_result\n",
    "\n",
    "    def q6(self):\n",
    "        \"\"\"\n",
    "        This function should perform unsupervised learning using LocallyLinearEmbedding in Sklearn. You can assume that\n",
    "        the function 'q1' has been called prior to calling this function.\n",
    "\n",
    "        :return: The function should return the data you visualize.\n",
    "        \"\"\"\n",
    "\n",
    "        res = np.zeros(1)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5fc807a-43ce-45a4-8eaf-81f17dd4614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest\n",
      "[[148. 121. 120. ... 106.  94. 104.]\n",
      " [ 56.  98.  83. ... 173. 127. 112.]\n",
      " [ 69.  93.  96. ... 204. 160. 150.]\n",
      " ...\n",
      " [ 32.  55.  80. ...  32.  53.  81.]\n",
      " [ 54.  71.  80. ...  53.  73.  82.]\n",
      " [102.  99. 108. ...  96.  96. 106.]]\n",
      "['AnnualCrop' 'AnnualCrop' 'AnnualCrop' ... 'SeaLake' 'SeaLake' 'SeaLake']\n"
     ]
    }
   ],
   "source": [
    "dataset = COC131()\n",
    "image_array, class_label = dataset.q1(\"Forest_1.jpg\")  # Change to an actual filename in your dataset\n",
    "print(class_label)\n",
    "images, labels = dataset.q1()\n",
    "print(images)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bcda5e-a7a9-4456-bbb7-b01ae13a3139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322c14b-0cdc-403e-b678-b36f14efe9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
