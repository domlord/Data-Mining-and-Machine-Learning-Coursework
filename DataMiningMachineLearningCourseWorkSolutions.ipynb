{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa42a1b-7834-453b-b54e-b244b6bd0fa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Please write the optimal hyperparameter values you obtain in the global variable 'optimal_hyperparm' below. This\n",
    "# variable should contain the values when I look at your submission. I should not have to run your code to populate this\n",
    "# variable.\n",
    "optimal_hyperparam = {}\n",
    "\n",
    "class COC131:\n",
    " def q1(self, filename=None):\n",
    "    \"\"\"\n",
    "    This function should be used to load the data. To speed-up processing in later steps, lower resolution of the\n",
    "    image to 32*32. The folder names in the root directory of the dataset are the class names. After loading the\n",
    "    dataset, you should save it into an instance variable self.x (for samples) and self.y (for labels). Both self.x\n",
    "    and self.y should be numpy arrays of dtype float.\n",
    "\n",
    "    :param filename: this is the name of an actual random image in the dataset. You don't need this to load the\n",
    "    dataset. This is used for testing the implementation.\n",
    "    :return res1: a one-dimensional numpy array containing the flattened low-resolution image in file 'filename'.\n",
    "    Flatten the image in the row major order. The dtype for the array should be float.\n",
    "    :return res2: a string containing the class name for the image in file 'filename'. This string should be the same as\n",
    "    one of the folder names in the originally shared dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get all subfolders within the dataset directory\n",
    "    subfolders = [entry.path for entry in os.scandir(\"../dataset\") if entry.is_dir()]\n",
    "\n",
    "    # Creates a list of all paths for the images stored within the ../dataset directory\n",
    "    image_paths = [os.path.join(subfolder, file)\n",
    "                 for subfolder in subfolders\n",
    "                 for file in os.listdir(subfolder) if file.endswith(('.jpg'))] # If the file is an image\n",
    "\n",
    "    # Processes each image within the dataset directory and stores it into the images array\n",
    "    self.x = np.array([np.array(Image.open(image_path).resize((32, 32)), dtype=float).flatten() for image_path in image_paths])\n",
    "\n",
    "    # Gets the name of the folder that the image is stored in (the classification of the image)\n",
    "    self.y = np.array([os.path.basename(os.path.dirname(image_path)) for image_path in image_paths])\n",
    "\n",
    "    # If a filename is provided, find and return it\n",
    "    if filename:\n",
    "        matching_image_path = next((image_path for image_path in image_paths if filename in image_path), None)\n",
    "        \n",
    "        if matching_image_path:\n",
    "            res1 = np.array(Image.open(matching_image_path).resize((32, 32)), dtype=float).flatten()\n",
    "            res2 = os.path.basename(os.path.dirname(matching_image_path))  # Extract class name\n",
    "            return res1, res2\n",
    "\n",
    "    return self.x, self.y\n",
    "     \n",
    "    def q2(self, inp):\n",
    "        \"\"\"\n",
    "        This function should compute the standardized data from a given 'inp' data. The function should work for a\n",
    "        dataset with any number of features.\n",
    "\n",
    "        :param inp: an array from which the standardized data is to be computed.\n",
    "        :return res2: a numpy array containing the standardized data with standard deviation of 2.5. The array should\n",
    "        have the same dimensions as the original data\n",
    "        :return res1: sklearn object used for standardization.\n",
    "        \"\"\"\n",
    "\n",
    "        standard_scaler = StandardScaler() # Creates a sklearn object used for standardisation\n",
    "        standardised_data = standardScaler.fit_transform(inp) # Standardises the data (setting the standard deviation to 1)\n",
    "\n",
    "        res1 = standard_scaler\n",
    "        res2 = standardised_data * 2.5 # Standardises the data to have a standard deviation of 2.5\n",
    "\n",
    "        return res2, res1\n",
    "\n",
    "\n",
    "        \n",
    "    def visualise_standardization(original_data, standardized_data):\n",
    "         \n",
    "         \"\"\"\n",
    "         Visualises original and standardised data side by side using boxplots.\n",
    "         :param original_data: numpy array of original dataset\n",
    "         :param standardized_data: numpy array of standardized dataset\n",
    "         \"\"\"\n",
    "         fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "         axs[0].boxplot(original_data, vert=True)\n",
    "         axs[0].set_title(\"Original Data Distribution\")\n",
    "         axs[0].set_xlabel(\"Features\")\n",
    "         axs[0].set_ylabel(\"Value\")\n",
    "    \n",
    "         axs[1].boxplot(standardized_data, vert=True)\n",
    "         axs[1].set_title(\"Standardized Data Distribution (std = 2.5)\")\n",
    "         axs[1].set_xlabel(\"Features\")\n",
    "         axs[1].set_ylabel(\"Value\")\n",
    "         \n",
    "    \n",
    "         plt.tight_layout()\n",
    "         plt.show()\n",
    "\n",
    "    def q3(self, test_size=None, pre_split_data=None, hyperparam=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function builds an MLP Classifier using the dataset loaded in function 'q1' and evaluates model\n",
    "        erformance. You can assume that the function 'q1' has been called prior to calling this function.\n",
    "        This function supports hyperparameter optimizations.\n",
    "        \n",
    "        :param test_size: The proportion of the dataset that should be reserved for testing. Should be a fraction between 0 and 1.\n",
    "        Default is 0.3 (30% for testing)\n",
    "        :param pre_split_data: Can be used to provide data already split into training and testing.\n",
    "        param hyperparam: Dictionary of hyperparameter values to be tested during optimization.\n",
    "        :return: The function returns 1 model object and 3 numpy arrays containing the loss, training accuracy,\n",
    "        and testing accuracy after each training iteration for the best model found.\n",
    "        \"\"\"\n",
    "        # Normalise the data using q2()\n",
    "        if self.scaler is None:\n",
    "            self.x, self.scaler = self.q2(self.x)\n",
    "\n",
    "        # Set default test size to 30% if none is provided\n",
    "        if test_size is None:\n",
    "            test_size = 0.3  # Default 70% training, 30% testing split\n",
    "    \n",
    "        # Train/test split if pre-split data is not provided\n",
    "        if pre_split_data:\n",
    "            x_train, x_test, y_train, y_test = pre_split_data\n",
    "        else:\n",
    "            x_train, x_test, y_train, y_test = train_test_split(self.x, self.y, test_size=test_size, random_state=1)\n",
    "    \n",
    "        # If hyperparameters are provided, use them directly\n",
    "        if hyperparam:\n",
    "            model = MLPClassifier(**hyperparam, random_state=1) # Set the random_state to 1 as a value to standardise the classifier generated\n",
    "            model.fit(x_train, y_train)\n",
    "            self.best_hyperparams = hyperparam  # Store the given hyperparameters\n",
    "        else:\n",
    "            # Define parameter grid for Grid Search (excluding alpha, which is tuned in q4)\n",
    "            param_grid = {\n",
    "                \"hidden_layer_sizes\": [(50,), (100,), (100, 50), (200,)],  # Different network sizes\n",
    "                \"learning_rate\": [\"constant\", \"adaptive\"],  # Learning rate type\n",
    "                \"solver\": [\"adam\", \"sgd\"]  # Optimisation solvers\n",
    "            }\n",
    "\n",
    "        # Initialise Grid Search with cross-validation\n",
    "        # This Grid Search creates a \n",
    "        grid_search = GridSearchCV(\n",
    "            MLPClassifier(random_state=1),  # Set the random_state to 1 as a value to standardise the classifier generated\n",
    "            param_grid)\n",
    "\n",
    "        # Perform Grid Search\n",
    "        grid_search.fit(x_train, y_train)\n",
    "\n",
    "        # Get the best model and hyperparameters\n",
    "        highest_accuracy_model = grid_search.best_estimator_\n",
    "        self.best_hyperparams = grid_search.best_params_\n",
    "    \n",
    "        # Extract performance metrics\n",
    "        loss_curve = np.array(model.loss_curve_)  # Loss during training\n",
    "        train_accuracy = np.array([accuracy_score(y_train, model.predict(x_train))])\n",
    "        test_accuracy = np.array([accuracy_score(y_test, model.predict(x_test))])\n",
    "    \n",
    "        return highest_accuracy_model, loss_curve, train_accuracy, test_accuracy\n",
    "\n",
    "    def q4(self):\n",
    "        \"\"\"\n",
    "        This function should study the impact of alpha on the performance and parameters of the model. For each value of\n",
    "        alpha in the list below, train a separate MLPClassifier from scratch. Other hyperparameters for the model can\n",
    "        be set to the best values you found in 'q3'. You can assume that the function 'q1' has been called\n",
    "        prior to calling this function.\n",
    "\n",
    "        :return: res should be the data you visualized.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Train/test split (same as q3)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(self.x, self.y, test_size=0.3, random_state=1)\n",
    "    \n",
    "        # Alpha values to test\n",
    "        alpha_values = [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 50, 100]\n",
    "    \n",
    "        train_accuracies = []\n",
    "        test_accuracies = []\n",
    "    \n",
    "        # Train a model for each alpha value\n",
    "        for alpha in alpha_values:\n",
    "            # Use best hyperparameters from q3(), but override alpha\n",
    "            hyperparams = self.best_hyperparams.copy()\n",
    "            hyperparams[\"alpha\"] = alpha\n",
    "    \n",
    "            model = MLPClassifier(**hyperparams, random_state=1)\n",
    "            model.fit(X_train, y_train)\n",
    "    \n",
    "            # Evaluate model\n",
    "            train_acc = accuracy_score(y_train, model.predict(x_train))\n",
    "            test_acc = accuracy_score(y_test, model.predict(x_test))\n",
    "    \n",
    "            train_accuracies.append(train_acc)\n",
    "            test_accuracies.append(test_acc)\n",
    "    \n",
    "            # Model parameters\n",
    "            weights = model.coefs_\n",
    "            biases = model.intercepts_\n",
    "\n",
    "            total_w = sum(w.size for w in weights)\n",
    "            total_b = sum(b.size for b in biases)\n",
    "    \n",
    "            total_weights.append(total_w)\n",
    "            total_biases.append(total_b)\n",
    "\n",
    "        # Store results in a dictionary\n",
    "        res = {\n",
    "        \"alpha_values\": alpha_values,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"test_accuracies\": test_accuracies,\n",
    "        \"total_weights\": total_weights,\n",
    "        \"total_biases\": total_biases,\n",
    "    }\n",
    "\n",
    "    return res\n",
    "\n",
    "    def q5(self):\n",
    "        \"\"\"\n",
    "        This function performs hypothesis testing to study the impact of using CV with and without Stratification\n",
    "        on the performance of MLPClassifier. Set other model hyperparameters to the best values obtained in the previous\n",
    "        questions. Use 5-fold cross validation for this question. You can assume that the function 'q1' has been called\n",
    "        prior to calling this function.\n",
    "    \n",
    "        :return: The function returns 4 items - the final testing accuracy for both methods of CV, p-value of the\n",
    "                 test and a string representing the result of hypothesis testing. The string can have only two possible values:\n",
    "                 'Splitting method impacted performance' or 'Splitting method had no effect'.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Use best hyperparameters from q3()\n",
    "        hyperparams = self.best_hyperparams.copy()\n",
    "    \n",
    "        # Define the model\n",
    "        model = MLPClassifier(**hyperparams, random_state=1)\n",
    "    \n",
    "        # Define 5-Fold CV (Without Stratification)\n",
    "        kf = KFold(shuffle=True, random_state=1)\n",
    "        cross_value_scores_kf = cross_val_score(model, self.x, self.y, cv=kf, n_jobs=-1)\n",
    "    \n",
    "        # Define 5-Fold Stratified CV\n",
    "        skf = StratifiedKFold(shuffle=True, random_state=1)\n",
    "        cross_value_scores_skf = cross_val_score(model, self.x, self.y, cv=skf, n_jobs=-1)\n",
    "    \n",
    "        # Compute mean testing accuracy\n",
    "        mean_accuracy_kf = np.mean(cv_scores_kf)\n",
    "        mean_accuracy_skf = np.mean(cv_scores_skf)\n",
    "    \n",
    "        # Perform paired t-test to compare distributions\n",
    "        t_stat, p_value = ttest_rel(cv_scores_kf, cv_scores_skf)\n",
    "    \n",
    "        # Determine hypothesis test result\n",
    "        alpha = 0.05  # Significance level\n",
    "        if p_value < alpha:\n",
    "            hypothesis_result = \"Splitting method impacted performance\"\n",
    "        else:\n",
    "            hypothesis_result = \"Splitting method had no effect\"\n",
    "    \n",
    "        return mean_acc_kf, mean_acc_skf, p_value, hypothesis_result\n",
    "        \n",
    "    def q6(self):\n",
    "        \"\"\"\n",
    "        This function performs unsupervised learning using LocallyLinearEmbedding in sklearn to reduce the dataset\n",
    "        to 2 dimensions. It returns the embedded data and corresponding labels to enable visualization of class separability.\n",
    "    \n",
    "        :return: Dictionary containing 2D embedded data and class labels.\n",
    "        \"\"\"\n",
    "    \n",
    "        if self.x is None or self.y is None:\n",
    "            raise ValueError(\"Dataset not loaded. Please call q1() first.\")\n",
    "    \n",
    "        # Apply LLE with custom n_neighbors, but default 2D output\n",
    "        lle = LocallyLinearEmbedding(n_neighbors=10, random_state=1)\n",
    "        embedded_data = lle.fit_transform(self.x)\n",
    "    \n",
    "        res = {\n",
    "            \"embedded_data\": embedded_data,\n",
    "            \"labels\": self.y\n",
    "        }\n",
    "    \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5fc807a-43ce-45a4-8eaf-81f17dd4614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest\n",
      "[[148. 121. 120. ... 106.  94. 104.]\n",
      " [ 56.  98.  83. ... 173. 127. 112.]\n",
      " [ 69.  93.  96. ... 204. 160. 150.]\n",
      " ...\n",
      " [ 32.  55.  80. ...  32.  53.  81.]\n",
      " [ 54.  71.  80. ...  53.  73.  82.]\n",
      " [102.  99. 108. ...  96.  96. 106.]]\n",
      "['AnnualCrop' 'AnnualCrop' 'AnnualCrop' ... 'SeaLake' 'SeaLake' 'SeaLake']\n"
     ]
    }
   ],
   "source": [
    "dataset = COC131()\n",
    "image_array, class_label = dataset.q1(\"Forest_1.jpg\")  # Change to an actual filename in your dataset\n",
    "print(class_label)\n",
    "images, labels = dataset.q1()\n",
    "print(images)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1bcda5e-a7a9-4456-bbb7-b01ae13a3139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [15 lines of output]\n",
      "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  rather than 'sklearn' for pip commands.\n",
      "  \n",
      "  Here is how to fix this error in the main use cases:\n",
      "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  - if the 'sklearn' package is used by one of your dependencies,\n",
      "    it would be great if you take some time to track which package uses\n",
      "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  - as a last resort, set the environment variable\n",
      "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \n",
      "  More information is available at\n",
      "  https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322c14b-0cdc-403e-b678-b36f14efe9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
